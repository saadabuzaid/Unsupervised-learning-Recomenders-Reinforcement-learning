{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Learning objectives:*\n",
    "* Understand key terms such as return, state, action, and policy as it applies to reinforcement learning\n",
    "* Understand the Bellman equations\n",
    "* Understand the state-action value function\n",
    "* Understand continuous state spaces\n",
    "* Build a deep Q-learning network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a machine learning technique to find the best action given current state, and it is the algorithm's responsibility to find the best action using a reward function.\n",
    "RL can handle complex environments where the optimal behavior is not easily defined or programmed explicitly.\n",
    "It allows agents to learn and adapt to new situations without human intervention.\n",
    "\n",
    "\n",
    "*Applications examples:*\n",
    "* Drones autonomus flying & robotics control\n",
    "* Train AI agnets for games (e.g. chess, ..etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the key components of RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent**: The decision-maker or learner that interacts with the environment. \\\n",
    "**Environment**: The world or situation the agent interacts with, providing observations (states) based on the agent's actions. \\\n",
    "**Action**: The choices the agent can make to influence the environment. \\\n",
    "**State**: The current situation or observation the agent has about the environment. \\\n",
    "**Reward**: The feedback signal the agent receives from the environment for taking an action (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the agent learn in RL?\n",
    "\n",
    "Unlike supervised learning, the agent doesn't have pre-defined instructions. It learns by:\n",
    "Interacting with the environment.\n",
    "Taking actions.\n",
    "Observing the rewards it receives.\n",
    "Updating its internal policy to favor actions that lead to higher rewards in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are some applications of RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is used in various fields such as:\n",
    "\n",
    "Training AI agents for games (e.g., AlphaGo)\n",
    "Robotics control (e.g., robot navigation)\n",
    "Resource management (e.g., resource allocation in a network)\n",
    "Recommendation systems (e.g., suggesting products based on user interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the different types of RL algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of RL algorithms exist depending on the environment and learning goals:\n",
    "\n",
    "Model-based RL: The agent builds a model of the environment to learn the optimal policy.\n",
    "Model-free RL: The agent learns directly from interacting with the environment without building a model.\n",
    "Value-based RL: The agent learns the value of different states or actions to choose the best ones.\n",
    "Policy-based RL: The agent directly learns the policy that maps states to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the challenges of RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL can be computationally expensive, especially for complex environments.\n",
    "Balancing exploration vs. exploitation:\n",
    "Exploration: Trying new actions to learn the environment.\n",
    "Exploitation: Choosing known good actions to maximize rewards.\n",
    "Defining the reward function:\n",
    "The reward function needs to be carefully designed to guide the agent towards the desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the epsilon-greedy policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon-greedy policy is a strategy for balancing exploration and exploitation in reinforcement learning. Exploration is essential for the agent to learn about the environment and discover potentially better rewards. However, relying solely on exploration can be inefficient. Exploitation allows the agent to focus on maximizing rewards based on its current knowledge. The epsilon-greedy policy balances these two aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the epsilon-greedy policy work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy defines how the agent chooses an action in a given state. In epsilon-greedy:\n",
    "\n",
    "Epsilon (ε): This is a probability (between 0 and 1) that the agent will explore by taking a random action. \\\n",
    "1 - Epsilon (1 - ε): This is the probability (1 minus epsilon) that the agent will exploit by taking the action it believes is currently the best (based on its learned values)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
