{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usupervised learning, recommendation systems, and reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning vs Unspervised learning\n",
    "With supervised learning, you have features and targets that your model can learn and validate the outputs compated to target. However, with unsupervised learning, you are giving the model data (features) with no targets (y), and the model will try to find something interesting about your data (can be pattern, structure, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Clustering algorithm?\n",
    "It is a unsupervised learning algorithm that will look into a dataset and try to split them into clusters based on structure/patterns\n",
    "\n",
    "**Applications exmaple** Grouping similar news, DNA anaylsis, and astronimical data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is k-mean clustering?\n",
    "The k-mean will create few points called cluster centroids at random position, and start assigning data to the closest cluster centroid to it; after that, it will compute the mean of the points that are assigned to it now, and this mean is its new position. We again assign data to the closest cluster, and calculate mean until the model converges, and there is no more changes either to the assignment of the data or the centriod position\n",
    "\n",
    "**Note:** If K cluster has no assigned data, you can delete the cluster.\n",
    "\n",
    "!(image of clustering centroids)[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means Optimization objective:\n",
    "We can optimise the k-means by using a cost function similar to what we have done before. The steps to optimise the k-means algorithm:\n",
    "1- Assign the data to the the clusters closest to them, this should decrease the cost initially\n",
    "2- Calculate distortion between the cluster and its assigned data points, and move the cluster to the new mean position according to its assigned data points, and repeat until convergence.\n",
    "\n",
    "**Note:** k-means cost should go down or stay the same, if it goes up this means there is a bug in the code\n",
    "\n",
    "Here is the cost function:\\\n",
    "![K-means cost function](/Resources/K-means%20cost%20function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing clusters:\n",
    "The 2 main questions when initializing a k-mean algorithm is how many clusters there should be, and what would be their initial values:\n",
    "\n",
    "- The number of K clusters is often ambigious, and the best way to decide the number of K cluster is really dependant on the business/purpose of the model. However, if you cannot decide the number clusters, you can use a function like the elbow method.\n",
    "\n",
    "- For the initial position for the clusters, a good starting point would be setting the cluster position to be the same as a random data point; setting clusters to be at the same position as a data point can sometimes cause the cluster to be stuck when trying to converge and group the data correctly, hence it is recommended to run different initial values for k clusters (between 50 - 1000) and get the best model with least cost (distortion).\n",
    "![Different iniital clusters values costs example](/Resources/different%20initial%20cluster%20values%20example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly detection\n",
    "\n",
    "It is an unsupervised learning algorithm where it detects the unusual/anomaly entry/data entry/event given datasets that are normal.\n",
    "One way to find an anomaly is by using the Gaussian (normal) distribution, and decide according to the data entry features whether this entry is within the usual region on the normal distribution graph or not. In order to use the Gaussian distribution, we need to find the following values: \\\n",
    "1- The mean of the distribution (Mu): This is the center of the distribution graph \\\n",
    "2- The standard deviation: the deviation between the center and the belt graph points. There is a formula for it \\\n",
    "3- Threshold (epsilon): A value in which if the data entry is less than the set threshold value, it is considered to be an anamoly \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if the data entry has more than 1 feature?\n",
    "We can calculate the probability of the X vector by multilpying the probablity of all its features.\n",
    "P(vector X) = P(X1) * P(X2) * .... * P(Xn)\n",
    "\n",
    "This new probability of vector X value is the what we compare to the threshold (epsilon) to detect whether this is an anomaly or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly detection algorithm steps:\n",
    "![Anomaly detection algorithm steps](/Resources/Anomaly%20detection%20algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly detection evaluation\n",
    "\n",
    "Although the anomaly detection algorithm is an unsupervised learning algorithm, still we need some labeled examples to validate and test. If there is no enough data, most people tend to use the anomaly labeled examples in the validation set, and have no test dataset, which makes it hard to evaluate how good the model is, and to avoid over fitting; still this is the best path to follow when you don't have much labeled anomaly examples to split between validation and test sets, which makes it a little bit risky. \\\n",
    "We will use the validation set to evaluate and tune the epsilone value (theshold). \\\n",
    "With anomaly detection systems, most probably the data will be skewed (big difference between positive and negative classes), so you can use metrics like calculating TP,FP,TN,FN, or metrics like precision/recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly detection vs supervised learning\n",
    "* Anomaly detection not only predicts anomalies based on previous seen/known anomalies, but will also try to figure out unseen/not known anomalies by figuring out the pattern.\n",
    "* Supervised learning will only predict anomalies seen/known before\n",
    "* Supervised learning excels with labeled data, offering clear classification of both normal and anomalous data points.\n",
    "* Anomaly detection works well with unlabeled data, identifying deviations from the learned normal behavior. It can potentially detect entirely new anomalies but might struggle with false positives\n",
    "\n",
    "Another key factor is whether you have a high enough proportion of anomaly (True) examples that training can be effective.\n",
    "\n",
    "If the dataset is small or skewed such that there are few anomaly examples, then a classifier is likely to get lower cost by just predicting the False condition all the time without much influence by the True examples.\n",
    "\n",
    "There are no hard limits for what makes a skewed dataset (because the total size of the dataset matters also), but one threshold you could consider is if True is less than 5% of the examples.\n",
    "\n",
    "For example, if you have 20 examples and only one is True, thatâ€™s a 1/20 ratio (5%). This dataset would work very badly in a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing what features to use\n",
    "\n",
    "Since we are using the gaussian disttribution to find the anomalies, the features used need to be gaussian representable by transforming any non-gaussian features data.\n",
    "Also, as we did before with supervised learning, feature engineering is another aspect that we need to consider, according to the business need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
